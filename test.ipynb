{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from ctm_dataloader import create_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset: (11314, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  target  \\\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "\n",
       "            target_names  \n",
       "0              rec.autos  \n",
       "1  comp.sys.mac.hardware  \n",
       "2  comp.sys.mac.hardware  \n",
       "3          comp.graphics  \n",
       "4              sci.space  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data = pd.read_csv(\"newsgroups_data.csv\")\n",
    "print(\"Shape of dataset:\", news_data.shape)\n",
    "\n",
    "news_data = news_data.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "documents = news_data.content\n",
    "target_labels = news_data.target\n",
    "target_names = news_data.target_names\n",
    "\n",
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing vocabulary:\n",
      "['also' 'article' 'believe' 'call' 'come' 'drive' 'even' 'file' 'find'\n",
      " 'first']\n",
      "Shape of the bag-of-words model: (11314, 50)\n",
      "Shape of data: torch.Size([32, 50])\n"
     ]
    }
   ],
   "source": [
    "# Create dataloader\n",
    "batch_size = 32\n",
    "train_loader, vocab_size = create_dataloader(documents, batch_size)\n",
    "\n",
    "for idx, data in enumerate(train_loader):\n",
    "    print(\"Shape of data:\", data.shape)\n",
    "    data_df = pd.DataFrame(data.numpy())\n",
    "    data_df.head()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sufficent statistics class for M-step\n",
    "\n",
    "class SufficientStats():\n",
    "    \"\"\"\n",
    "    Stores statistics about variational parameters during E-step in order\n",
    "    to update CtmModel's parameters in M-step.\n",
    "\n",
    "    `self.mu_stats` contains sum(lamda_d)\n",
    "\n",
    "    `self.sigma_stats` contains sum(I_nu^2 + lamda_d * lamda^T)\n",
    "\n",
    "    `self.beta_stats[i]` contains sum(phi[d, i] * n_d) where nd is the vector\n",
    "    of word counts for document d.\n",
    "\n",
    "    `self.numtopics` contains the number of documents the statistics are build on\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_topics, vocab_size):\n",
    "        self.num_docs = 0\n",
    "        self.num_topics = num_topics\n",
    "        self.vocab_size = vocab_size\n",
    "        self.beta_stats = torch.zeros([num_topics, vocab_size])\n",
    "        self.mu_stats = torch.zeros(num_topics)\n",
    "        self.sigma_stats = torch.zeros([num_topics, num_topics])\n",
    "\n",
    "    def update(self, lamda, nu2, phi, doc):\n",
    "        \"\"\"\n",
    "        Given optimized variational parameters, update statistics\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # update mu_stats\n",
    "        self.mu_stats += lamda\n",
    "\n",
    "        # update \\beta_stats[i], 0 < i < self.numtopics\n",
    "        for n, c in enumerate(doc):\n",
    "            for i in range(self.num_topics):\n",
    "                self.beta_stats[i, n] += c * phi[n, i]\n",
    "                \n",
    "        # update \\sigma_stats\n",
    "        self.sigma_stats += torch.diag(nu2) + torch.matmul(lamda, torch.t(lamda))\n",
    "\n",
    "        self.num_docs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19.6484)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_topics = 10\n",
    "vocab_size = 100\n",
    "lamda = torch.randn(num_topics)\n",
    "nusqrd = torch.ones(num_topics)\n",
    "\n",
    "torch.sum(torch.exp(torch.add(lamda, nusqrd, alpha = 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTM model class - contains all the parameters and methods for the model\n",
    "\n",
    "class CTM():\n",
    "    \n",
    "    def __init__(self, num_topics, vocab_size, num_epochs, batch_size, lr,\n",
    "                 tol_em = 1e-6, tol_es = 1e-5, max_iter = 10):\n",
    "        super(CTM, self).__init__()\n",
    "\n",
    "        self.num_topics = num_topics # k - number of topics\n",
    "        self.vocab_size = vocab_size # V - size of vocabulary\n",
    "        self.tol_em = tol_em # relative change we need to achieve in E-step\n",
    "        self.tol_es = tol_es # relative change we need to achieve in Expectation-Maximization\n",
    "        self.max_iter = max_iter # maximum number of iterations\n",
    "        self.num_epochs = num_epochs # number of epochs\n",
    "        self.batch_size = batch_size # batch size\n",
    "        self.lr = lr # learning rate\n",
    "\n",
    "        # Model parameters\n",
    "        self.mu = torch.zeros(self.num_topics) #k-dimensional mean\n",
    "        self.sigma = torch.eye(self.num_topics) # k x k symmetrical matrix - covariance matrix\n",
    "        self.sigma_inv = torch.inverse(self.sigma) # k x k symmetrical matrix - inverse of covariance matrix\n",
    "        self.beta = torch.ones(num_topics, vocab_size) # k x V matrix - global topic-word distribution\n",
    "        self.theta = torch.rand(num_topics) # k-dimensional vector - document-topic distribution\n",
    "\n",
    "        # Variantional parameters\n",
    "        self.lamda = nn.Parameter(torch.zeros(self.num_topics)) # k-dimensional vector - gaussian mean\n",
    "        self.nusqrd = nn.Parameter(torch.ones(self.num_topics)) # k-dimensional vector - gaussian variance\n",
    "        self.phi = nn.Parameter(1/float(self.num_topics) * torch.ones(self.vocab_size, self.num_topics)) # V x k matrix - local topic-word distribution\n",
    "        self.zeta = nn.Parameter(torch.sum(torch.exp(torch.add(self.lamda, self.nusqrd, alpha = 0.5)))) # scalar - variational parameter\n",
    "        \n",
    "        #Debug check\n",
    "        if(0 in self.beta):\n",
    "            print(\"There is a zero in beta 1\")\n",
    "        \n",
    "    def train(self, corpus):\n",
    "        # Two phases, E and then M, where we try to maximize bound on log prob\n",
    "        # Repeat until difference is less than TOL_EM\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            print(f'----- Training epoch {i+1} of {self.max_iter} -----')\n",
    "            \n",
    "            old_bound = self.corpus_bound(corpus)\n",
    "            print(f'Initial bound at the start of train_loop: {old_bound}')\n",
    "            \n",
    "            statistics = self.e_phase(corpus)\n",
    "            self.m_phase(statistics)\n",
    "\n",
    "            delta = (self.corpus_bound(corpus) - old_bound)/old_bound\n",
    "\n",
    "            print(\"delta\",delta)\n",
    "            print(\"non-normalized delta\", (delta * old_bound))\n",
    "\n",
    "            # max just to make sure delta is positive, could use abs\n",
    "            if max(-delta, delta) < self.tol_em:\n",
    "                print('Training converged')\n",
    "                writer.close()\n",
    "                break\n",
    "            \n",
    "    def log_bound(self, doc, zeta = None, lamda = None, nusqrd = None, phi = None):\n",
    "        \"\"\" Bound the log probability of a document using Jensen's inequality \"\"\"\n",
    "        if lamda is None:\n",
    "            lamda = self.lamda\n",
    "        if nusqrd is None:\n",
    "            nusqrd = self.nusqrd\n",
    "        if phi is None:\n",
    "            phi = self.phi\n",
    "        if zeta is None:\n",
    "            zeta = self.zeta\n",
    "            \n",
    "        N = sum(doc)\n",
    "        bound  = 0.0\n",
    "        \n",
    "        # Part 1 of equation 4\n",
    "        bound += 0.5 * torch.log(torch.linalg.det(self.sigma_inv))\n",
    "        bound += -0.5 * self.num_topics * np.log(2*np.pi)\n",
    "        bound += -0.5 * torch.trace(torch.diag(nusqrd) @ self.sigma_inv) \\\n",
    "            + torch.t(lamda - self.mu) @ self.sigma_inv @ (lamda - self.mu)\n",
    "           \n",
    "        # Part 2 partial of equation 4 \n",
    "        \"\"\"Eq [log p(zn | η)]\"\"\"\n",
    "        expect = torch.sum(torch.exp(lamda + 0.5 * nusqrd)).item()\n",
    "        bound += (N * (-1/zeta * expect + 1 - torch.log(zeta))).item()\n",
    "\n",
    "        # Part 4 partial of equation 4\n",
    "        \"\"\"Entropy term\"\"\"\n",
    "        sum_1 = 0\n",
    "        for topic in range(self.num_topics):\n",
    "            sum_1 += torch.log(nusqrd[topic]) + np.log(2 * np.pi) + 1\n",
    "            \n",
    "        bound += sum_1.item()\n",
    "        \n",
    "        # Part 2 partial, 3 and 4 partial of equation 4\n",
    "        for word, count in enumerate(doc):\n",
    "            for topic in range(self.num_topics):\n",
    "                bound += (count*phi[word,topic] * (lamda[topic] + torch.log(self.beta[topic,word]) - torch.log(phi[word,topic]))).item()\n",
    "                \n",
    "                # debugging check 1\n",
    "                if(torch.tensor(bound).isnan()):\n",
    "                    print(\"Bound is Nan\")\n",
    "                    print(\"first: \", count*phi[word,topic])\n",
    "                    print(\"second: \", (lamda[topic] + torch.log(self.beta[topic,word]) - torch.log(phi[word,topic])))\n",
    "                    print(\"beta: \", self.beta[topic,word])\n",
    "                    print(\"beta log: \", torch.log(self.beta[topic,word]))\n",
    "                    print(\"phi log: \", torch.log(phi[word,topic]))\n",
    "                    exit()\n",
    "                    \n",
    "                # debugging check 2\n",
    "                if((count*phi[word,topic]).isnan()):\n",
    "                    print(\"first isNan\")\n",
    "                    print(\"phi\", phi)\n",
    "                    print(\"element\",phi[word,topic])\n",
    "                \n",
    "                # debugging check 3\n",
    "                if ((lamda[topic] + torch.log(self.beta[topic,word]) - torch.log(phi[word,topic]))).isnan():\n",
    "                    print(\"second isNan\")\n",
    "                    print(lamda[topic])\n",
    "                    print(self.beta[topic,word])\n",
    "                    print(phi[word,topic])\n",
    "                    exit()\n",
    "        \n",
    "        return -bound\n",
    "    \n",
    "    def corpus_bound(self, corpus):\n",
    "        \"\"\" Compute the bound on the log probability of the corpus \"\"\"\n",
    "        return sum([self.log_bound(doc) for doc in corpus])\n",
    "            \n",
    "    def e_phase(self, corpus):\n",
    "        \"\"\"Coordinate ascent optimization of the variational parameters\"\"\"\n",
    "        print('Starting E-phase')\n",
    "        statistics = SufficientStats(self.num_topics, self.vocab_size)\n",
    "        \n",
    "        for i, doc in enumerate(corpus):\n",
    "            if i % 8 == 0:\n",
    "                print(f'Processing document {i+1} to {i+8} of {len(corpus)} documents')\n",
    "            \n",
    "            model = copy.deepcopy(self)\n",
    "            model.variational_inference(doc)\n",
    "            \n",
    "            statistics.update(self.lamda, self.nusqrd, self.phi, doc)\n",
    "        print(f'Beta at the end of e_phase: {self.beta.shape}')\n",
    "        #Debug check\n",
    "        if(0 in self.beta):\n",
    "            print(\"THERE IS A ZERO IN BETA 1\")\n",
    "            \n",
    "        return statistics\n",
    "    \n",
    "    def variational_inference(self, doc):\n",
    "        # bound = self.log_bound(doc)\n",
    "        # new_bound = bound\n",
    "        \n",
    "        def target(doc, zeta, lamda, nusqrd, phi):\n",
    "            return self.log_bound(doc, zeta = zeta, lamda = lamda, nusqrd = nusqrd, phi = phi)\n",
    "        \n",
    "        optimizer = torch.optim.Adam([self.zeta, self.lamda, self.nusqrd, self.phi], lr = self.lr)\n",
    "        \n",
    "        # for _ in range(self.max_iter):\n",
    "        #     self.max_zeta()\n",
    "        #     self.max_phi(doc)\n",
    "        #     self.max_lamda(doc, self.lamda, self.phi)\n",
    "        #     self.max_nusqrd(doc, self.nusqrd)\n",
    "            \n",
    "        for epoch in range(self.num_epochs):\n",
    "            loss = target(doc, self.zeta, self.lamda, self.nusqrd, self.phi)\n",
    "            writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        writer.flush()\n",
    "            \n",
    "            # bound, new_bound = new_bound, self.log_bound(doc)\n",
    "            # relative_change = abs((new_bound - bound) / bound)\n",
    "            \n",
    "            # if (relative_change < self.tol_es):\n",
    "            #     break\n",
    "        \n",
    "    # def e_phase_part1(self):\n",
    "    #     \"\"\"Eq [log p(η | µ, Σ)]\"\"\"\n",
    "    #     part_1 = 0.5 * torch.log(torch.linalg.det(self.sigma_inv))\n",
    "    #     part_2 = -0.5 * self.num_topics * np.log(2*np.pi)\n",
    "    #     part_3 = -0.5 * torch.trace(torch.diag(self.nusqrd) @ self.sigma_inv) \\\n",
    "    #         + torch.t(self.lamda - self.mu) @ self.sigma_inv @ (self.lamda - self.mu)\n",
    "    #     return part_1 + part_2 + part_3\n",
    "    \n",
    "    # def e_phase_part2(self, doc):\n",
    "    #     \"\"\"Eq [log p(zn | η)]\"\"\"\n",
    "    #     part_1 = 0.0\n",
    "    #     for n,c in enumerate(doc):\n",
    "    #         for i in range(self.num_topics):\n",
    "    #             part_1 += c*self.phi[n,i] * self.lamda[i]\n",
    "    #     part_2 = -(1/self.zeta) * torch.sum(torch.exp(self.lamda + 0.5*self.nusqrd)) + 1 - torch.log(self.zeta)\n",
    "    #     return part_1 + part_2\n",
    "    \n",
    "    # def e_phase_part3(self, doc):\n",
    "    #     \"\"\"Eq [log p(wn | zn, β)]\"\"\"\n",
    "    #     sum = 0\n",
    "    #     for n,c in enumerate(doc):\n",
    "    #         for i in range(self.num_topics):\n",
    "    #             sum += c * torch.sum(self.phi[n,i] * torch.log(self.beta[i,n]))\n",
    "    #     return sum\n",
    "    \n",
    "    # def e_phase_part4(self):\n",
    "    #     sum_1 = 0\n",
    "    #     for topic in range(self.num_topics):\n",
    "    #         sum_1 += torch.log(self.nusqrd[topic]) + np.log(2 * np.pi) + 1\n",
    "    #     sum_2 = 0\n",
    "    #     for word in range(self.vocab_size):\n",
    "    #         for topic in range(self.num_topics):\n",
    "    #             sum_2 += self.phi[word, topic] * torch.log(self.phi[word, topic])\n",
    "    #     return 0.5 * sum_1 - sum_2\n",
    "    \n",
    "    def max_zeta(self):\n",
    "        \"\"\"Maximize zeta\"\"\"\n",
    "        return torch.sum(torch.exp(torch.add(self.lamda, self.nusqrd, alpha = 0.5)))\n",
    "    \n",
    "    def max_phi(self, corpus):\n",
    "        phi_norm = 0.0\n",
    "        for n, _ in enumerate(corpus):\n",
    "            for i in range(self.num_topics):\n",
    "                phi_norm = sum([torch.exp(self.lamda[i]) * self.beta[i,n]])\n",
    "            for i in range(self.num_topics):\n",
    "                self.phi[n, i] = torch.exp(self.lamda[i]) * self.beta[i, n] / phi_norm\n",
    "                \n",
    "    def max_lamda(self, doc, lamda, phi):\n",
    "        \n",
    "        def target(doc, lamda, phi):\n",
    "            return self.log_bound(doc, lamda = lamda, phi = phi)\n",
    "        \n",
    "        optimizer = torch.optim.Adam([doc, lamda], maximize = True)\n",
    "        \n",
    "        for _ in range(10):\n",
    "            optimizer.zero_grad()\n",
    "            loss = target(doc, lamda, phi)\n",
    "            # loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    \n",
    "    def max_nusqrd(self, doc, nusqrd):\n",
    "        \n",
    "        def target(doc, nusqrd):\n",
    "            return self.log_bound(doc, nusqrd = nusqrd)\n",
    "        \n",
    "        optimizer = torch.optim.Adam([doc, nusqrd], maximize = True)\n",
    "        \n",
    "        for _ in range(10):\n",
    "            optimizer.zero_grad()\n",
    "            loss = target(doc, nusqrd)\n",
    "            # loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    def m_phase(self, sstats):\n",
    "        \"\"\"Maximize the parameters of the model\"\"\"\n",
    "        print('Starting M-phase')\n",
    "        if(0 in sstats.beta_stats):\n",
    "            print(\"MPHASE ZERO\")\n",
    "        \n",
    "        for i in range(self.num_topics):\n",
    "            beta_norm = torch.sum(sstats.beta_stats[i])\n",
    "            self.beta[i] = sstats.beta_stats[i] / beta_norm\n",
    "        \n",
    "        self.mu = sstats.mu_stats / sstats.num_docs\n",
    "        self.sigma = sstats.sigma_stats + torch.matmul(self.mu, torch.t(self.mu))\n",
    "        self.sigma_inv = torch.inverse(self.sigma)\n",
    "        #Debug check\n",
    "        if(0 in self.beta):\n",
    "            print(\"THERE IS A ZERO IN BETA 1\")\n",
    "\n",
    "#num_topic is the number of topics to find\n",
    "def runModel(train_loader, num_topics, num_epochs, batch_size, lr):\n",
    "\n",
    "    # Create dataloader\n",
    "    # train_loader, vocab_size = create_dataloader(data, batch_size)\n",
    "\n",
    "    torch.set_printoptions(threshold=20000) #lets us print larger tensors to terminal for debugging\n",
    "\n",
    "    model = CTM(num_topics, vocab_size, num_epochs, batch_size, lr)\n",
    "    \n",
    "    for i, batch in enumerate(train_loader):\n",
    "        print(f'--------- Training batch {i+1} ---------')\n",
    "        model.train(batch)\n",
    "        \n",
    "    print(f'Final probabilies: {model.beta}')\n",
    "    \n",
    "    # save the model.beta\n",
    "    torch.save(model.beta, \"beta.pt\")\n",
    "    \n",
    "    corrCoef = torch.corrcoef(model.beta)\n",
    "    sns.heatmap(corrCoef, annot=True)\n",
    "    plt.show(block=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "num_topics = 10\n",
    "batch_size = 32\n",
    "lr = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = pd.read_csv('newsgroups_data.csv')\n",
    "news_data = news_data.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "runModel(train_loader = train_loader, num_topics = num_topics, num_epochs = num_epochs, batch_size = batch_size, lr = lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MSIT5670",
   "language": "python",
   "name": "msit5670"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
